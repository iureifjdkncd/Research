# Augmented Condition Vectorê¸°ë°˜ Custom LSTM-CVAE

## 1) Augmented Condition Vector

### ğŸ“Œ ì •ì˜
ê¸°ì¡´ Condition Vector(ëª©í‘œê°’ $c_t$)ì— **ìµœê·¼ ë‹¤ë³€ëŸ‰ ì…ë ¥ ì‹œí€€ìŠ¤ì˜ í†µê³„ì  íŠ¹ì§•(í‰ê· , í‘œì¤€í¸ì°¨)**ì„ ì¶”ê°€í•˜ì—¬  
Generative ëª¨ë¸ì´ ì…ë ¥ ì‹œê³„ì—´ì˜ **ì¶”ì„¸(Trend)**ì™€ **ë³€ë™ì„±(Volatility)**ì„ í•¨ê»˜ ì¸ì‹í•˜ë„ë¡ ê°•í™”í•œ êµ¬ì¡°.

---

### ğŸ”¹ ìƒì„± ê³¼ì •
ìµœê·¼ $N'$ê°œì˜ ì…ë ¥ ì‹œí€€ìŠ¤

$$
\hat{x}_{t-N':t-1} \in \mathbb{R}^{N' \times T \times F}
$$

ê° Featureë³„ í†µê³„ì¹˜ ê³„ì‚°(Target ë³€ìˆ˜ ì œì™¸):

$$
\mu_t = \frac{1}{N'} \sum_{i=t-N'}^{t-1} \hat{x}^i
$$

$$
\sigma_t = \frac{1}{N'} \sum_{i=t-N'}^{t-1} (\hat{x}^i - \mu_t)^2
$$

- $\mu_t$: ê° Featureì˜ í‰ê·  â†’ **Trend ë°˜ì˜**  
- $\sigma_t$: ê° Featureì˜ í‘œì¤€í¸ì°¨ â†’ **Volatility ë°˜ì˜**

> $N' = 0$ (ì´ì „ ë°ì´í„° ì—†ìŒ) â†’ $\mu_t = 0$, $\sigma_t = 0$

---

### ğŸ”¹ Condition Vector ì¶”ê°€ìš© Scalar ë³€í™˜
ëª¨ë“  Featureì— ëŒ€í•œ í‰ê· ê°’ì„ êµ¬í•´ ìŠ¤ì¹¼ë¼ í˜•íƒœë¡œ ì¶•ì•½:

$$
\bar{\mu}_t = \frac{1}{F} \sum_{j=1}^F [\mu_t]_j
$$

$$
\bar{\sigma}_t = \frac{1}{F} \sum_{j=1}^F [\sigma_t]_j
$$

---

### ğŸ”¹ ìµœì¢… Augmented Condition Vector ìƒì„±
ì›ë˜ì˜ ì¡°ê±´ ë²¡í„° $c_t$ì™€ ìœ„ ë‘ ìŠ¤ì¹¼ë¼ë¥¼ ì—°ê²°:

$$
\tilde{c}_t = \mathrm{concat}(c_t, \bar{\mu}_t, \bar{\sigma}_t)
$$

- ê²°ê³¼ ì°¨ì›: $D + 2$  
- ìµœê·¼ ì‹œê³„ì—´ íë¦„ + ëª©í‘œê°’ì„ í•¨ê»˜ ë°˜ì˜

---

## 2) LSTM-CVAE ê³¼ì • ë° ìˆ˜ì‹

LSTM-CVAEëŠ” **Conditional Variational AutoEncoder(CVAE)**ì— LSTM êµ¬ì¡°ë¥¼ ê²°í•©í•˜ì—¬,  
ì…ë ¥ ì‹œê³„ì—´ íŠ¹ì„±($X$)ê³¼ ì¡°ê±´ ì •ë³´($Y$)ë¥¼ í•¨ê»˜ ì¸ì½”ë”©Â·ë””ì½”ë”©í•˜ë„ë¡ ì„¤ê³„ë¨.

---

### 2.1 Encoder (ì¡°ê±´ë¶€ LSTM ì¸ì½”ë”)

#### **ì…ë ¥ ì •ë³´ í™•ì¥**
ê° ì‹œì  ì…ë ¥ $x_t$ì— ë™ì¼í•œ $\tilde{c}_t$ë¥¼ ë¶™ì—¬ ì¡°ê±´ í¬í•¨:

$$
x'_t = [x_t; \tilde{c}_t]
$$

- ì°¨ì›: $\mathbb{R}^{T \times (F + D_c)}$, $D_c = D + 2$

---

#### **Gaussian Noise ì¶”ê°€ (ê°•ê±´ì„± í™•ë³´)**
$$
\tilde{c}_t \leftarrow \tilde{c}_t + \epsilon, \quad \epsilon \sim \mathcal{N}(0, \sigma_{\text{noise}}^2 I)
$$

---

#### **LSTM ì¸ì½”ë”©**
$$
h_{\mathrm{enc}} = \mathrm{LSTM}(x'_{1:T})
$$

---

#### **Latent Vector ë¶„í¬ ì¶”ì •**
$$
\mu = W_\mu h_{\mathrm{enc}} + b_\mu
$$

$$
\log \sigma^2 = W_{\log\mathrm{var}} h_{\mathrm{enc}} + b_{\log\mathrm{var}}
$$

---

### 2.2 Latent Sampling (Reparameterization Trick)
$$
z = \mu + \sigma \odot \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)
$$

---

### 2.3 Decoder (ì¡°ê±´ë¶€ LSTM ë””ì½”ë”)

#### **ì ì¬ë²¡í„° íˆ¬ì˜**
$$
h_z = \phi(W_z z + b_z)
$$
(ì‹œí€€ìŠ¤ ê¸¸ì´ $T$ë§Œí¼ ë°˜ë³µ)

---

#### **ì¡°ê±´ ê²°í•©**
$$
h_t^{(0)} = [h_{z,t}; \tilde{c}_t], \quad t = 1, \dots, T
$$

---

#### **LSTM ë³µì›**
$$
h_{\mathrm{dec}} = \mathrm{LSTM}(h_{1:T}^{(0)})
$$

---

#### **ì¶œë ¥ ë³€í™˜**
$$
\hat{x}_t = W_{\mathrm{out}} h_{\mathrm{dec},t} + b_{\mathrm{out}}
$$

---

#### **Post-hoc Gaussian Noise ì¶”ê°€**
$$
\hat{x}_{1:T} \leftarrow \hat{x}_{1:T} + \epsilon, \quad \epsilon \sim \mathcal{N}(0, \sigma_{\text{noise}}^2 I)
$$

---

### 2.4 Loss Function (ELBO ê¸°ë°˜)

#### **Reconstruction Loss (MAE)**
$$
L_{\mathrm{recon}} = \frac{1}{T} \sum_{t=1}^T \| x_t - \hat{x}_t \|_1
$$

#### **KL Divergence**
$$
L_{\mathrm{KL}} = D_{\mathrm{KL}}\big(q(z|x_t, \tilde{c}_t) \,\|\, p(z|\tilde{c}_t)\big)
$$

#### **KL Annealing**
$$
L_{\mathrm{total}} = L_{\mathrm{recon}} + \lambda_{\mathrm{KL}} L_{\mathrm{KL}}, \quad \lambda_{\mathrm{KL}} \in [0,1]
$$

#### **Target-Aware Penalty**
$$
L_{\mathrm{weighted}} = \big(1 + I(c_t \ge \tau) \cdot p\big) \, L_{\mathrm{total}}
$$

---

## ğŸ“Œ ì •ë¦¬
- **Augmented Condition Vector**  
  â†’ ëª©í‘œê°’($Y$) + ìµœê·¼ ì…ë ¥ ì‹œê³„ì—´($X$)ì˜ í†µê³„ íŠ¹ì§•(í‰ê· Â·í‘œì¤€í¸ì°¨) ì œê³µ â†’ **ì‹œê³„ì—´ íŒ¨í„´ ë°˜ì˜ë ¥ ê°•í™”**
- **LSTM-CVAE**  
  â†’ Condition Vectorë¥¼ ì¸ì½”ë”Â·ë””ì½”ë” ëª¨ë‘ì— ì‚½ì…  
  â†’ KL ì •ê·œí™”, ë…¸ì´ì¦ˆ, ê°€ì¤‘ ì†ì‹¤ë¡œ **í˜„ì‹¤ì„±Â·ë³€ë™ì„± ìœ ì§€ ì‹œí€€ìŠ¤ ìƒì„±**

